{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7615be6",
   "metadata": {},
   "source": [
    "# **LangChain Interview Preparation Guide (3+ Years Experience)**\n",
    "\n",
    "## ðŸ“‹ **Core Concepts Deep Dive**\n",
    "\n",
    "### 1. **Architecture & Components**\n",
    "```python\n",
    "# LangChain's modular architecture example\n",
    "from langchain.schema import BaseMemory, BaseOutputParser\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.agents import Tool, AgentExecutor\n",
    "\n",
    "# Custom components demonstrate deep understanding\n",
    "class CustomMemory(BaseMemory):\n",
    "    \"\"\"Implementation showing understanding of memory management\"\"\"\n",
    "    pass\n",
    "\n",
    "class CustomOutputParser(BaseOutputParser):\n",
    "    \"\"\"Custom output parsing shows advanced knowledge\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### 2. **Advanced Project Structure**\n",
    "```\n",
    "my-langchain-app/\n",
    "â”œâ”€â”€ agents/\n",
    "â”‚   â”œâ”€â”€ custom_agent.py\n",
    "â”‚   â””â”€â”€ agent_factory.py\n",
    "â”œâ”€â”€ chains/\n",
    "â”‚   â”œâ”€â”€ sequential_workflows.py\n",
    "â”‚   â””â”€â”€ custom_chains.py\n",
    "â”œâ”€â”€ memory/\n",
    "â”‚   â””â”€â”€ enhanced_memory.py\n",
    "â”œâ”€â”€ tools/\n",
    "â”‚   â”œâ”€â”€ database_tools.py\n",
    "â”‚   â””â”€â”€ api_tools.py\n",
    "â”œâ”€â”€ utils/\n",
    "â”‚   â””â”€â”€ prompt_optimizer.py\n",
    "â””â”€â”€ main.py\n",
    "```\n",
    "\n",
    "## ðŸš€ **Advanced Project Examples**\n",
    "\n",
    "### **Project 1: Enterprise RAG System with LangChain**\n",
    "```python\n",
    "# Advanced RAG with multiple retrieval techniques\n",
    "from langchain.retrievers import MultiQueryRetriever, ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.retrievers.ensemble import EnsembleRetriever\n",
    "from langchain.retrievers.merger_rewarder import MergerRetriever\n",
    "\n",
    "class EnterpriseRAGSystem:\n",
    "    def __init__(self, documents):\n",
    "        self.vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "        self.retriever = self._create_advanced_retriever()\n",
    "        \n",
    "    def _create_advanced_retriever(self):\n",
    "        # Multiple retrieval strategies\n",
    "        base_retriever = self.vectorstore.as_retriever(search_type=\"mmr\")\n",
    "        multi_query = MultiQueryRetriever.from_llm(\n",
    "            retriever=base_retriever, \n",
    "            llm=llm\n",
    "        )\n",
    "        \n",
    "        # Compression and filtering\n",
    "        compressor = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.7)\n",
    "        compression_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor, \n",
    "            base_retriever=multi_query\n",
    "        )\n",
    "        \n",
    "        return compression_retriever\n",
    "    \n",
    "    async def query_with_fallback(self, query: str, max_retries: int = 3):\n",
    "        \"\"\"Advanced error handling and retry logic\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                return await self.retriever.aget_relevant_documents(query)\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise e\n",
    "                await asyncio.sleep(2 ** attempt)\n",
    "```\n",
    "\n",
    "### **Project 2: Multi-Agent Workflow System**\n",
    "```python\n",
    "from langchain.agents import AgentType, initialize_agent, Tool\n",
    "from langchain.agents import AgentExecutor, OpenAIFunctionsAgent\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "class MultiAgentOrchestrator:\n",
    "    def __init__(self):\n",
    "        self.agents = self._initialize_specialized_agents()\n",
    "        \n",
    "    def _initialize_specialized_agents(self):\n",
    "        # Data analysis agent\n",
    "        data_tools = [Tool(\n",
    "            name=\"data_analyzer\",\n",
    "            func=self._analyze_data,\n",
    "            description=\"Analyze dataset and provide insights\"\n",
    "        )]\n",
    "        \n",
    "        data_agent = initialize_agent(\n",
    "            data_tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Research agent  \n",
    "        research_tools = [Tool(\n",
    "            name=\"web_researcher\",\n",
    "            func=self._research_online,\n",
    "            description=\"Research information online\"\n",
    "        )]\n",
    "        \n",
    "        research_agent = initialize_agent(\n",
    "            research_tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'data_analyst': data_agent,\n",
    "            'researcher': research_agent\n",
    "        }\n",
    "    \n",
    "    def orchestrate_workflow(self, task: str):\n",
    "        \"\"\"Orchestrate multiple agents for complex tasks\"\"\"\n",
    "        # Agent collaboration logic\n",
    "        if \"analyze\" in task and \"data\" in task:\n",
    "            return self.agents['data_analyst'].run(task)\n",
    "        elif \"research\" in task or \"find information\" in task:\n",
    "            return self.agents['researcher'].run(task)\n",
    "        else:\n",
    "            # Sequential agent execution\n",
    "            research_result = self.agents['researcher'].run(f\"Research: {task}\")\n",
    "            analysis_result = self.agents['data_analyst'].run(\n",
    "                f\"Analyze this information: {research_result}\"\n",
    "            )\n",
    "            return analysis_result\n",
    "```\n",
    "\n",
    "## ðŸ”§ **Advanced Technical Concepts**\n",
    "\n",
    "### **Custom Memory Management**\n",
    "```python\n",
    "from langchain.memory import ConversationBufferWindowMemory, EntityMemory\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List\n",
    "\n",
    "class EnhancedConversationMemory:\n",
    "    def __init__(self):\n",
    "        self.short_term = ConversationBufferWindowMemory(k=5)\n",
    "        self.long_term = EntityMemory(llm=llm)\n",
    "        self.context_memory = {}\n",
    "        \n",
    "    def add_context(self, key: str, context: Dict):\n",
    "        \"\"\"Add business context to memory\"\"\"\n",
    "        self.context_memory[key] = context\n",
    "        \n",
    "    def get_relevant_context(self, query: str) -> Dict:\n",
    "        \"\"\"Retrieve relevant business context\"\"\"\n",
    "        # Implement semantic search for context\n",
    "        return {\n",
    "            'user_preferences': self._find_relevant_preferences(query),\n",
    "            'business_rules': self._find_relevant_rules(query)\n",
    "        }\n",
    "```\n",
    "\n",
    "### **Performance Optimization**\n",
    "```python\n",
    "# Caching and performance optimization\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "import langchain\n",
    "import sqlite3\n",
    "\n",
    "# Enable caching\n",
    "langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\n",
    "\n",
    "class OptimizedLangChainService:\n",
    "    def __init__(self):\n",
    "        self._setup_optimizations()\n",
    "        \n",
    "    def _setup_optimizations(self):\n",
    "        # Batch processing for multiple queries\n",
    "        self.batch_size = 10\n",
    "        \n",
    "        # Connection pooling for database tools\n",
    "        self.db_pool = self._create_connection_pool()\n",
    "        \n",
    "    async def process_batch_queries(self, queries: List[str]):\n",
    "        \"\"\"Process multiple queries efficiently\"\"\"\n",
    "        results = []\n",
    "        for i in range(0, len(queries), self.batch_size):\n",
    "            batch = queries[i:i + self.batch_size]\n",
    "            batch_results = await asyncio.gather(*[\n",
    "                self._process_single_query(query) for query in batch\n",
    "            ])\n",
    "            results.extend(batch_results)\n",
    "        return results\n",
    "```\n",
    "\n",
    "## ðŸŽ¯ **Interview Questions & Answers**\n",
    "\n",
    "### **Technical Depth Questions:**\n",
    "\n",
    "1. **\"How do you handle conversation memory in large-scale applications?\"**\n",
    "   ```python\n",
    "   # Answer with code example\n",
    "   class ScalableMemorySolution:\n",
    "       def __init__(self):\n",
    "           self.redis_cache = RedisCache()  # For distributed caching\n",
    "           self.memory_ttl = 3600  # 1 hour expiration\n",
    "           \n",
    "       def store_conversation(self, session_id: str, messages: List):\n",
    "           \"\"\"Store with compression and TTL\"\"\"\n",
    "           compressed = self._compress_messages(messages)\n",
    "           self.redis_cache.setex(\n",
    "               f\"conv:{session_id}\", \n",
    "               self.memory_ttl, \n",
    "               compressed\n",
    "           )\n",
    "   ```\n",
    "\n",
    "2. **\"Explain agent tool usage with error handling\"**\n",
    "   ```python\n",
    "   # Advanced tool implementation with error handling\n",
    "   class RobustTool:\n",
    "       def __init__(self, max_retries=3):\n",
    "           self.max_retries = max_retries\n",
    "           \n",
    "       @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "       async def execute_with_retry(self, input_str: str):\n",
    "           try:\n",
    "               return await self._execute_tool(input_str)\n",
    "           except RateLimitError:\n",
    "               raise\n",
    "           except Exception as e:\n",
    "               self._log_error(e)\n",
    "               raise ToolExecutionError(f\"Tool failed: {str(e)}\")\n",
    "   ```\n",
    "\n",
    "### **System Design Questions:**\n",
    "\n",
    "3. **\"Design a LangChain system for customer support\"**\n",
    "   ```python\n",
    "   class CustomerSupportSystem:\n",
    "       def __init__(self):\n",
    "           self.classifier_chain = self._create_intent_classifier()\n",
    "           self.specialized_chains = {\n",
    "               'billing': self._create_billing_chain(),\n",
    "               'technical': self._create_technical_chain(),\n",
    "               'general': self._create_general_chain()\n",
    "           }\n",
    "           \n",
    "       def route_query(self, query: str):\n",
    "           intent = self.classifier_chain.run(query)\n",
    "           return self.specialized_chains[intent].run(query)\n",
    "   ```\n",
    "\n",
    "4. **\"How do you ensure data privacy in LangChain applications?\"**\n",
    "   ```python\n",
    "   class SecureLangChainImplementation:\n",
    "       def __init__(self):\n",
    "           self.encryption = DataEncryption()\n",
    "           self.anonymizer = PIIAnonymizer()\n",
    "           \n",
    "       def process_sensitive_data(self, text: str):\n",
    "           # Anonymize before processing\n",
    "           anonymized = self.anonymizer.anonymize(text)\n",
    "           result = self.llm_chain.run(anonymized)\n",
    "           # Re-identify if needed\n",
    "           return self.anonymizer.deanonymize(result)\n",
    "   ```\n",
    "\n",
    "## ðŸ“Š **Performance Monitoring & Logging**\n",
    "\n",
    "```python\n",
    "# Advanced monitoring setup\n",
    "from prometheus_client import Counter, Histogram\n",
    "import logging\n",
    "\n",
    "class MonitoredLangChainApp:\n",
    "    def __init__(self):\n",
    "        self.query_counter = Counter('langchain_queries_total', 'Total queries')\n",
    "        self.latency_histogram = Histogram('langchain_latency_seconds', 'Query latency')\n",
    "        self.setup_logging()\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    @contextmanager\n",
    "    def track_performance(self, query_type: str):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            latency = time.time() - start_time\n",
    "            self.latency_histogram.observe(latency)\n",
    "            self.query_counter.inc()\n",
    "            self.logger.info(f\"{query_type} query took {latency:.2f}s\")\n",
    "```\n",
    "\n",
    "## ðŸ”„ **Advanced Deployment Patterns**\n",
    "\n",
    "```python\n",
    "# Containerized LangChain application\n",
    "# Dockerfile snippet\n",
    "\"\"\"\n",
    "FROM python:3.9-slim\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "COPY . .\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "\n",
    "# Kubernetes deployment with resource management\n",
    "\"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: langchain-app\n",
    "spec:\n",
    "  replicas: 3\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: langchain\n",
    "        resources:\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "        env:\n",
    "        - name: OPENAI_API_KEY\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: api-secrets\n",
    "              key: openai-key\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "## ðŸ§ª **Testing Strategies**\n",
    "\n",
    "```python\n",
    "# Comprehensive testing suite\n",
    "import pytest\n",
    "from unittest.mock import Mock, patch\n",
    "\n",
    "class TestLangChainApplications:\n",
    "    @pytest.mark.asyncio\n",
    "    async def test_rag_retrieval_accuracy(self):\n",
    "        \"\"\"Test RAG system retrieval accuracy\"\"\"\n",
    "        test_queries = [\"product pricing\", \"return policy\"]\n",
    "        expected_min_precision = 0.8\n",
    "        \n",
    "        for query in test_queries:\n",
    "            results = await rag_system.retrieve_documents(query)\n",
    "            precision = self._calculate_precision(results, query)\n",
    "            assert precision >= expected_min_precision\n",
    "            \n",
    "    @patch('langchain_openai.OpenAI')\n",
    "    def test_llm_caching(self, mock_llm):\n",
    "        \"\"\"Test that LLM responses are properly cached\"\"\"\n",
    "        mock_llm.return_value = Mock()\n",
    "        response1 = chain.run(\"test query\")\n",
    "        response2 = chain.run(\"test query\")  # Should use cache\n",
    "        mock_llm.assert_called_once()  # LLM should be called only once\n",
    "```\n",
    "\n",
    "This preparation guide demonstrates **deep technical expertise** with LangChain, covering advanced patterns, production-ready code, and architectural considerations that interviewers expect from senior candidates with 3+ years of experience."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

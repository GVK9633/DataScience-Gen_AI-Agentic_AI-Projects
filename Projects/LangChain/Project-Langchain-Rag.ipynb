{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbc9582",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1NEVcOr56vYT3FiE2eUTW_0natI1Rd--_?authuser=1#scrollTo=fJOHF3hG7iwy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279a779",
   "metadata": {},
   "source": [
    "Of course! Let's break down each of these packages, explaining what they are and how they work together in a typical LangChain-based Generative AI application, like the retail Order Management System (OMS) we discussed.\n",
    "\n",
    "### The Big Picture: The RAG Pipeline\n",
    "\n",
    "These packages form the backbone of a **Retrieval-Augmented Generation (RAG)** system. Here’s the high-level flow they enable:\n",
    "\n",
    "1.  **Ingest & Parse:** Load and parse your PDF documents (e.g., product manuals, inventory reports) using `unstructured` and `poppler-utils`.\n",
    "2.  **Chunk:** Split the large text into manageable pieces using `langchain-text-splitters`.\n",
    "3.  **Embed:** Convert each text chunk into a numerical vector (embedding) using `sentence-transformers`.\n",
    "4.  **Store & Index:** Save those vectors in a specialized database for fast searching, using `chromadb`.\n",
    "5.  **Retrieve & Generate:** When a user asks a question, the system finds the most relevant text chunks and sends them to a powerful LLM (via `langchain-groq`) to generate a grounded, accurate answer.\n",
    "\n",
    "`langchain-community` and `langchain-chroma` provide the LangChain-specific glue to make all these steps easy to code.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `chromadb` → The Vector Database for Storing Embeddings\n",
    "\n",
    "*   **What it is:** An open-source, lightweight vector database. It's purpose-built for storing and querying vector embeddings.\n",
    "*   **Analogy:** Think of it like a specialized library. A normal database stores rows and columns of text/numbers. Chroma stores **points in multi-dimensional space** (vectors). It's incredibly fast at finding the \"closest\" points to a given query point.\n",
    "*   **Why it's needed:** You can't just dump thousands of vectors into a CSV file or SQL database and expect to quickly find the ones most similar to a new query. Chroma handles this \"similarity search\" efficiently.\n",
    "*   **Key Function:** `collection.query(query_embeddings=..., n_results=5)` returns the 5 text chunks whose vectors are most similar to your question.\n",
    "\n",
    "### 2. `langchain-groq` → Integration for Groq LLMs\n",
    "\n",
    "*   **What it is:** A LangChain integration package for models hosted on the **Groq API**. Groq is famous for its incredibly fast inference speeds (often 100s of tokens per second).\n",
    "*   **Why it's special:** You use this instead of, say, `openai` or `anthropic` packages. It lets you use LangChain to easily call powerful LLMs like Mixtral or Llama 3 hosted on Groq's super-fast hardware.\n",
    "*   **Key Function:** Wraps the Groq API into a LangChain `LLM` object, so you can do:\n",
    "    ```python\n",
    "    from langchain_groq import ChatGroq\n",
    "    llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)\n",
    "    response = llm.invoke(\"What's the weather like?\")\n",
    "    ```\n",
    "\n",
    "### 3. LangChain Core Components (`langchain-community`, `langchain-chroma`, `langchain-text-splitters`)\n",
    "\n",
    "These are all part of the modular LangChain library.\n",
    "\n",
    "*   **`langchain-community`**: A \"catch-all\" package for third-party integrations that aren't in the core `langchain` package. This is where community-contributed code for tools, LLMs, vectorstores, and document loaders lives. You'll use imports from here all the time.\n",
    "    *   *Example:* `from langchain_community.document_loaders import PyPDFLoader`\n",
    "\n",
    "*   **`langchain-chroma`**: The LangChain **integration** for the `chromadb` vector database. It provides a wrapper class (`Chroma`) that understands the LangChain API, making it easy to use Chroma within your LangChain chains without writing low-level Chroma DB code.\n",
    "    *   *Example:* `from langchain_chroma import Chroma` + `Chroma.from_documents(documents, embeddings)`\n",
    "\n",
    "*   **`langchain-text-splitters`**: Crucial tools for breaking down large documents into smaller chunks. You can't feed a 100-page PDF to an LLM. This package provides smart algorithms to split text while trying to preserve semantic meaning (e.g., not splitting a sentence in half).\n",
    "    *   *Example:* `from langchain_text_splitters import RecursiveCharacterTextSplitter`\n",
    "\n",
    "### 4. `transformers` & `sentence-transformers` → Embedding Models\n",
    "\n",
    "*   **`transformers` (by Hugging Face):** The foundational library providing thousands of pre-trained models for NLP tasks (text classification, summarization, question answering, and crucially, **creating embeddings**).\n",
    "*   **`sentence-transformers`**: A fantastic wrapper library built on top of `transformers` that is specifically designed and optimized for creating **sentence embeddings**. Its models (like `all-MiniLM-L6-v2`) are trained to make sure similar sentences have similar vector representations.\n",
    "*   **Their Role:** They are the \"encoder.\" They take text and convert it into the numerical vectors that `chromadb` stores.\n",
    "    *   *Example:* `from sentence_transformers import SentenceTransformer` + `model.encode(\"Some text\")`\n",
    "\n",
    "### 5. `unstructured` & `unstructured[pdf]` → Parsing PDFs into Text\n",
    "\n",
    "*   **What it is:** A powerful library designed to pre-process and parse various file formats (PDFs, PPTX, HTML, Images, etc.) into clean, structured text.\n",
    "*   **Why it's needed:** PDFs are a nightmare for computers. Text can be in columns, images can contain text, and formatting is complex. `unstructured` handles this complexity and extracts the raw text, which is what your LLM needs.\n",
    "*   **`unstructured[pdf]`**: Installing the `[pdf]` \"extra\" includes all the necessary dependencies for PDF processing (which includes `poppler-utils`).\n",
    "\n",
    "### 6. `poppler-utils` → The Engine for PDF Processing\n",
    "\n",
    "*   **What it is:** A set of command-line tools for rendering PDFs. It's the open-source backend that many libraries (like `unstructured`, `pdf2image`, `PyMuPDF`) rely on to actually read and interpret the PDF file format.\n",
    "*   **Its Role:** It's a **system dependency**. `unstructured` uses it under the hood. You need it installed on your system for PDF parsing to work.\n",
    "\n",
    "---\n",
    "\n",
    "### How They Work Together: A Code Snippet\n",
    "\n",
    "```python\n",
    "# 1. IMPORTS (Showcasing the packages)\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader  # Uses unstructured & poppler\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter   # Splits text\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings      # Uses sentence-transformers\n",
    "from langchain_chroma import Chroma                                   # Vector DB integration\n",
    "from langchain_groq import ChatGroq                                   # LLM Provider\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 2. LOAD & CHUNK (unstructured, text-splitters)\n",
    "loader = UnstructuredPDFLoader(\"inventory_report.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. EMBED & STORE (sentence-transformers, chromadb)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./db\")\n",
    "\n",
    "# 4. RETRIEVE & GENERATE (chromadb, langchain-groq)\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=vector_db.as_retriever())\n",
    "\n",
    "# 5. QUERY!\n",
    "question = \"What were the top-selling products in Q4?\"\n",
    "answer = qa_chain.run(question) # Chroma finds relevant chunks, Groq generates answer\n",
    "print(answer)\n",
    "```\n",
    "\n",
    "This stack is a perfect recipe for building a powerful, private, and cost-effective Q&A system over your own documents without retraining a model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
